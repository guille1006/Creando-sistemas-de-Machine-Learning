{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4a88f2",
   "metadata": {},
   "source": [
    "Este primer capitulo se centrará en explicar los aspectos básicos detras del uso del Machine Learning\n",
    "\n",
    "## Cuando usar ML\n",
    "Como sabemos, los algorítmos de ML no son una herramienta mágica capaz de resolver todos los problemas de la misma manera, incluso en problemas que pueden ser solucionados con ML, es posible que este proceso no sea el más óptimo\n",
    "\n",
    "\n",
    "<div style=\"border: 10px solid black; padding: 10px;diplay: inline-block; margin:20px 0; width:fit-content;\">\n",
    "    Machine learning is an approach to (1)learn (2)complex patterns from (3)existing data and use these patterns to make (4)predictions on (5)unseen data.\n",
    "</div>\n",
    "\n",
    "1. Learn. el sistema es capaz de aprender.\n",
    "2. Complex patterns. Las soluciones con ML solo son posible si existe un patrón desde donde aprender, si nosotros tiramos un dado (juega únicamente el azar, no hay patrón), ML no sería capaz de predecir el siguiente resultado. \n",
    "3. Existing data. Como los ML aprenden de los datos, por lo que esto será la base de donde empezarán a aprender. Si los datos son malos o no son suficientes, nuestro sistema tendrá errores. \n",
    "4. Predictions. El fin que tiene nuestro programa es darnos una predicción. \n",
    "5. Unseen data. Son los datos donde queremos predecir su variable dependiente, es importante indicar que los datos no vistos deberán compartir los patrones que los datos con los que se ha entrenado el modelo. \n",
    "\n",
    "Debido a como aprender los ML actualmente, nuestro proyecto podrá brillar si además cumplen las siguientes caracteristicas:\n",
    "6. Es repetitivo. \n",
    "7. El coste de predicciones erroneas es barato.\n",
    "8. Es escalable. \n",
    "9. Puede haber patrones que cambien constantemente. Este aspecto puede diferir mucho del punto 5, pero ten en cuenta que nosotros querremos entender los patrones que cambian en tareas como en detectores de span de mail(ahora un spam puede venir del principe de arabia, pero mañana podría ser de Fernando Alonso), si nosotros intentasemos hardcodearlo, tendriamos que estar escribiendolo constantemente. \n",
    "\n",
    "## ML in research VS in production\n",
    "| Aspecto | Investigación | Producción |\n",
    "|-------|--------------|------------|\n",
    "| Objetivo principal | Rendimiento de modelos de última generación en conjuntos de datos de referencia (benchmarks) | Cumplir requisitos del mundo real y del negocio |\n",
    "| Requisitos | Enfoque principalmente académico | Diferentes partes interesadas tienen distintos requisitos |\n",
    "| Prioridad computacional | Entrenamiento rápido y alto rendimiento experimental | Inferencia rápida, baja latencia |\n",
    "| Datos | Estáticos | Cambian constantemente |\n",
    "| Equidad (Fairness) | A menudo no es una prioridad | Debe ser considerada |\n",
    "| Interpretabilidad | A menudo no es una prioridad | Debe ser considerada |\n",
    "\n",
    "\n",
    "## Objetivos del ML y el trabajo\n",
    "Primero deberemos considerar los objetivos del proyecto de ML propuesto. \n",
    "\n",
    "Cuando los data scientist se enfocan en las métricas de rendimineto, latencia, precisión... Las compañias no se fijan en estas métricas tan bonitas, según el economista Milton Friedman, su objetivo es maximizar el benecifico de los accionistas. Esto significa que buscan incrementar los beneficios, ya sea directamente o indirectamente: incrementando las ventas, reduciendo gastos, mayor satisfacción del cliente, más tiempo en gastado en la página web\n",
    "\n",
    "## Requerimientos para los sistemas ML\n",
    "No podemos decir que hemos diseñado satisfactoriamente un sistema de ML sin saber que requerimentos el sistema debe satisfacer, estos sistemas deberan tener: \n",
    "\n",
    "### Reliability / Fiabilidad\n",
    "El sistema deberá poder desempeñar su función correctamente a un alto nivel incluso en escenarios adversos (fallos en el hardware/software o errores humanos).\n",
    "\n",
    "Uno de los temas de la fiabilidad es como saber si un sistema está fallando. Los ML no son capaces de avisarte si están fallando, ellos simplemente te devolverán el resultado más probable que hayan calculado. \n",
    "\n",
    "### Scalability /  Escalabilidad\n",
    "Nuestro sistema de ML deberá ser capaz de escalar sus recursos, ya sea un *up-scaling* (exapndir recursos) o *down-scaling* (reducir recursos cuando no se neceisten). Esta caracteristica es muy importante a la hora de entender los momentos picos del sistema. \n",
    "\n",
    "### Maintainability / Mantenibilidad\n",
    "Consiste es en estructurar correctamente las cargas de trabajo y crear una infraestructura donde todos los contribudores puedan acceder. Algunos ejemplos son: \n",
    "* Documentar el código.\n",
    "* Código, datos y artefactos deberán estar versionados. \n",
    "* Los modelos deberán ser facilmente reproducibles.\n",
    "* Cuando un problema ocurra, varias personas podrán trabajar en solucionarlo.\n",
    "\n",
    "### Adaptability / Adaptabilidad\n",
    "Adaptarse al cambio de las distribuciones de datos y requisitos de negocios. El sistema debera ser capaz de descubrir aspectos para mejorar el desempeño y permitir actualizaciones sin que se caiga el servicio. Esto está muy relacionado con la **Mantenibilidad**\n",
    "\n",
    "\n",
    "## Función Objetivo --> Loss function\n",
    "Todos los modelos de ML necesitan una función objetivo que sirva para guiar el proceso de aprendizaje. Es también llamada *loss function* porque el objetivo de esta función es minimizar (u optimizar) la **pérdida** causada por las predicciones erroneas. Esta pérdida puede ser calculada mediante la comparación de las salidas dadas por el modelo con las etiquetas reales. \n",
    "\n",
    "### Desaparejar objetivos. \n",
    "Imagina que tienes que hacer un sistema en el cual tienes varios objetivos, y además, dos de ellos están enfrentados (esto significa que la disminución en pérdida en uno, conlleva un aumento en la pérdida del otro objetivo) lo que deberás hacer es crear una función que englobe ambas pérdidas:\n",
    "$$\n",
    "loss = \\alpha loss_1 + \\beta loss_2\n",
    "$$\n",
    "Con la función de pérdida anterior podrás manejar cuanta importancia le darás a cada una de las pérdidas, creando así una función donde su objetivo será minimizar el valor de la pérdida de ese sumatorio. La parte negativa de esta nueva función es que cada vez que querramos cambiar el valor de $\\alpha$ o $\\beta$ deberemos volver a entrenar todo el modelo. \n",
    "\n",
    "Otra forma de tratar con este problema es entrenar dos modelos donde cada uno optimize una función de pérdida, y que al final combinemos las salidas de ambos modelos, para así tunear tanto $\\alpha$ como $\\beta$ sin que haga falta volver a entrenar.\n",
    "$$\n",
    "\\alpha loss_1 + \\beta loss_2\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce719065",
   "metadata": {},
   "source": [
    "# Fundamentos de la Ingeniería de datos\n",
    "## Datos \n",
    "Prácticamente el elemento más importante a la hora de entrenar un modelo son los propios **datos**. Y esto lo saben las compañías, ya que se interesan más en manejar y en mejorar los datos que en mejorar el algorítmo. \n",
    "\n",
    "Pero hay que tener en cuenta que aunque gran parte del progreso en Deep Learning en las últimas decadas es gracias a las grandes cantidades de datos, más datos no implican mejor desempeño del modelo. Datos de mala calidad, como datos desactualizados o con etiquetas incorrectas pueden dañar este desempeño.\n",
    "\n",
    "## Fuentes de datos \n",
    "* Input data. Datos explicitemente introducidos por el usuario. Hay que tener cuidado con estos datos porque si de la más remota manera de que el usuario pueda cometer un error, lo cometerá.\n",
    "* System generated data. Estos son los datos recopilados por el sistema. Estos son datos que dificilmente pueden estar mal formateados, puesto que es el administrador quien lo diseña. \n",
    "* Internal databases. Es donde se almacena y gestiona los activos.\n",
    "* Third-Party Data. Firs-Party Data son los datos que se recolecta de los usuarios o clientes. Second-Party Data son los datos que recolecta otra compañía sobre sus clientes, estos datos pagas tu por usarlos. Third-Party data son los datos que tu compañía recolecta del público que no es directamente sus clientes.\n",
    "\n",
    "## Formato de los datos\n",
    "**Data Serialization** es el proceso de convertir una estructura de datos en un formato que pueda ser almacenado o transmitido y reconstruido luego. Hay muchos detalles a tener en cuenta: la facilidad de entendimiento humano, patrones de acceso, texto o binario...\n",
    "\n",
    "| Format   | Binary/Text | Human-readable | Example use cases                    |\n",
    "|----------|-------------|----------------|--------------------------------------|\n",
    "| JSON     | Text        | Yes            | Everywhere                           |\n",
    "| CSV      | Text        | Yes            | Everywhere                           |\n",
    "| Parquet  | Binary      | No             | Hadoop, Amazon Redshift              |\n",
    "| Avro     | Binary      | No             | Hadoop                               |\n",
    "| Protobuf | Binary      | No             | Google, TensorFlow (TFRecord)        |\n",
    "| Pickle   | Binary      | No             | Python, PyTorch serialization        |\n",
    "\n",
    "\n",
    "### JSON\n",
    "**JSON** JavaScript Object Notation, que aunque derive de JavaScript es un lenguaje independiente. Fácilmente leíble. Basado en el paradigma del par *key-value*. \n",
    "Como JSON es omnipresente, los problemas que puede llegar a tener también se siente que está en todas partes. Una vez que se ha definido el esquema de un archivo JSON es muy doloroso volver a redefinirlo.\n",
    "\n",
    "### CSV y Parquet\n",
    "Son dos de los formato de datos más usados actualmente, se diferencian entre ellos en la forma de almacenar los datos.\n",
    "* **CSV** es *row-major* que significa que los datos son guardados en memoria en filas. \n",
    "* **Parquet** es *column-major* en este caso los datos son almacenados en memoria en columnas. \n",
    "\n",
    "Como los ordenadores modernos procesan los datos secuencialmente más rápido que los datos no secuenciales, se espera que los formatos *row-major* deberían ser más rápidos. \n",
    "\n",
    "Una curiosidad importante es saber que **numpy** es *row-major* mientras que **pandas** es *column-major*. Por eso las funciones de columnas son mucho más rápidas en **pandas**. \n",
    "\n",
    "### Formato de texto o binario. \n",
    "**CSV** y **JSON** son archivos de texto, mientras que **Parquet** son archivos binarios. Los archivos de texto son guardados en texto plano, lo cual implica que puede ser leído por una persona fácilmente. Los archivos binarios tan solo contienen 0 y 1, y son pensados para ser leídos por ordenadores. \n",
    "Un punto a favor de los archivos binarios es que ocupan bastante menos que los archivos de texto.\n",
    "\n",
    "## Modelos de datos\n",
    "Los modelos de datos describen como los datos están representados.\n",
    "### Modelos relacionales. SQL\n",
    "En este modelo los datos son organizados por **relaciones**, cada relación es un conjunto de tuplas. Cada seria de la tabla es una tupla. Las relaciones (columnas) y registros pueden estar desordenados.\n",
    "\n",
    "Una de las desventajas de este tipo de modelo es que te obliga a seguir un esquema muy restrictivo\n",
    "\n",
    "### Modelos no relacionales. NoSQL (Not only SQL)\n",
    "* **Document Model**. Se centra en los casos donde los datos vienen en documentos auto-contenidos y la relación entre estos datos son raras. Suele estar escrito en JSON, XML, BSON. Cada documento tiene una llave (id) único que le represente.\n",
    "* **Graph Model**. Se centra en los casos donde las relaciones entre los datos son mas importantes y comunes. Los grafos consisten en nodos y enlaces, donde estos enlaces representan la relación entre los datos. Como las reslaciones son modeladas explícitamente en los grafos, es más rápido sacar datos usando las relaciones\n",
    "\n",
    "## Data Storage Engines and Processing\n",
    "Los *motores de almacenamiento* también conocidos como databases son la implementación de como los datos son almacenados y se obtienen estos. Típicamente hay dos trabajos para lo que estos están optimizados: \n",
    "* Procesamiento transacional\n",
    "* Procesamiento analítico\n",
    "\n",
    "### Procesamiento transaccional. Online Transaction Processing\n",
    "Este procesamiento se refiere al acto de comprar o vender algo, en el caso de las bases de datos, la transacción será el envio/recibo de cualquier dato. Estas transacciones son generadas, actualizadas y borradas cuando se necesite.\n",
    "\n",
    "Como estas transacciones involucran usuarios, tienen que ser *rápidos*(**baja latencia**) y **alta disponibilidad**. Cuando se habla de las bases de datos transacionales se suele pensar en *ACID*.\n",
    "* *Atomicidad*. Que se garantize que todos los pasos en la trnasacción se completen como un grupo. Si algún paso de la transacción no se cumple, entonces toda la trnsacción fallará.\n",
    "* *Consistencia*. Que se garantice que todas las transacciones entrantes deberán seguir unas reglas predefinidas. \n",
    "* *Isolation*. Que se garantice que dos transacciones se puedan realizar al mismo tiempo de forma independiente. Por ejemplo que dos usuarios entren a la base sin que uno pese a otro.\n",
    "* *Durabilidad*. Que se garantice que una vez la transacción ha sido realizada, que se mantenga realizada incluso en caso de que el sistema se rompa. Por ejemplo pides un Uber y se te acaba la batería.\n",
    "\n",
    "No hace falta que todas las bases cumplan con todos los pasos anteriores ya que pueden ser demasiado restrictivos. En el caso de que no se cumplan se llamarán *BASE*: *Basically Available*\n",
    "\n",
    "### Procesamiento anlítico. Online Analytical Processing.\n",
    "En el caso anterior, como las transacciones son realizadas en paralelo, estas se suelen guardar como *row-major*, esto significa que sacar información que requiera *aggregaciones* serán poco eficientes. Aquí entra la OLAP, las cuales son eficientes con consultas que permitan buscar data desde distintos puntos de vista.\n",
    "\n",
    "\n",
    "### Actualidad. \n",
    "Actualmente ambos tipos de bases de datos están desactualizados, ahora existen motores que son bases de datos transaccionales pero que manejan consultas analíticas. \n",
    "\n",
    "## ETL: Extract, Transform and Load\n",
    "En los primeros tiempos de las base de datos, los datos eran estructurados. Cuando los datos son **extrídos** de diferentes fuentes, primero debian ser **transformados** al formato deseado antes de ser **cargado**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898cd8b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
